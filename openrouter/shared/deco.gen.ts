// Generated types - do not edit manually

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

export type String = string;

export interface DECO_CHAT_OAUTH_STARTInput {
  returnUrl: String;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

export type String_1 = string;
export type Array = String_1[];

export interface DECO_CHAT_OAUTH_STARTOutput {
  stateSchema?: unknown;
  scopes?: Array;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

export interface DECO_CHAT_STATE_VALIDATIONInput {
  state?: unknown;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

export type Boolean = boolean;

export interface DECO_CHAT_STATE_VALIDATIONOutput {
  valid: Boolean;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

export interface DECO_CHAT_VIEWS_LISTInput {}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

export type String_2 = string;
export type String_3 = string;
export type String_4 = string;
export type String_5 = string;
export type String_6 = string;
export type String_7 = string;
export type String_8 = string;
export type String_9 = string;
export type String_10 = string;
export type Array_2 = String_10[];
export type String_11 = string;
export type String_12 = "none" | "open" | "autoPin";
export type Array_1 = Object[];

export interface DECO_CHAT_VIEWS_LISTOutput {
  views: Array_1;
}
export interface Object {
  id?: String_2;
  name?: String_3;
  title: String_4;
  description?: String_5;
  icon: String_6;
  url?: String_7;
  mimeTypePattern?: String_8;
  resourceName?: String_9;
  tools?: Array_2;
  prompt?: String_11;
  installBehavior?: String_12;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

export interface GET_USERInput {}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

export type String_13 = string;
export type StringNull = String_14 | Null;
export type String_14 = string;
export type Null = null;
export type StringNull_1 = String_15 | Null_1;
export type String_15 = string;
export type Null_1 = null;
export type String_16 = string;

export interface GET_USEROutput {
  id: String_13;
  name: StringNull;
  avatar: StringNull_1;
  email: String_16;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * Array of messages in the conversation. Each message has a role (user/assistant/system) and content.
 *
 * @minItems 1
 */
export type Array_3 = [Object_1, ...Object_1[]];
/**
 * Message role
 */
export type String_17 = "user" | "assistant" | "system";
/**
 * Message content
 */
export type String_18 = string;
/**
 * Optional name for the message sender
 */
export type String_19 = string;
/**
 * Model ID to use (e.g., 'openai/gpt-4o', 'anthropic/claude-3.5-sonnet'). Use 'openrouter/auto' for automatic selection. Default: openrouter/auto
 */
export type String_20 = string;
export type String_21 = string;
/**
 * Fallback chain: array of model IDs to try in sequence if the primary model fails (e.g., ['openai/gpt-4o', 'anthropic/claude-3.5-sonnet'])
 */
export type Array_4 = String_21[];
/**
 * Sampling temperature (0-2). Higher values make output more random. Default: 1
 */
export type Number = number;
/**
 * Maximum tokens to generate in the completion
 */
export type Number_1 = number;
/**
 * Nucleus sampling parameter (0-1). Alternative to temperature.
 */
export type Number_2 = number;
/**
 * Reduce repetition of tokens by frequency (-2 to 2)
 */
export type Number_3 = number;
/**
 * Reduce repetition of topics (-2 to 2)
 */
export type Number_4 = number;
export type String_22 = string;
export type String_23 = string;
export type Array_5 = String_23[];
export type String_24 = "json_object";
/**
 * Sort providers by this preference
 */
export type String_25 = "price" | "throughput" | "latency";
export type String_26 = string;
/**
 * Only use these specific providers (e.g., ['OpenAI', 'Anthropic'])
 */
export type Array_6 = String_26[];
export type String_27 = string;
/**
 * Exclude these providers from selection
 */
export type Array_7 = String_27[];
/**
 * Require that providers support all requested parameters
 */
export type Boolean_1 = boolean;
/**
 * Allow fallback to other providers on failure
 */
export type Boolean_2 = boolean;
/**
 * Unique identifier for your end-user (helps with abuse detection)
 */
export type String_28 = string;

export interface CHAT_COMPLETIONInput {
  messages: Array_3;
  model?: String_20;
  models?: Array_4;
  temperature?: Number;
  maxTokens?: Number_1;
  topP?: Number_2;
  frequencyPenalty?: Number_3;
  presencePenalty?: Number_4;
  /**
   * Stop sequences to end generation early
   */
  stop?: String_22 | Array_5;
  responseFormat?: Object_2;
  provider?: Object_3;
  user?: String_28;
}
export interface Object_1 {
  role: String_17;
  content: String_18;
  name?: String_19;
}
/**
 * Request JSON output format. Note: Only supported by some models. Check model details.
 */
export interface Object_2 {
  type: String_24;
}
/**
 * Provider routing preferences to optimize selection by price, speed, or specific providers
 */
export interface Object_3 {
  sort?: String_25;
  only?: Array_6;
  exclude?: Array_7;
  requireParameters?: Boolean_1;
  allowFallbacks?: Boolean_2;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * Generation ID (use with GET_GENERATION for details)
 */
export type String_29 = string;
/**
 * Actual model that generated the response
 */
export type String_30 = string;
/**
 * The generated text response
 */
export type String_31 = string;
/**
 * Why generation stopped: 'stop' (natural end), 'length' (hit token limit), 'content_filter' (moderated), etc.
 */
export type String_32 = string;
/**
 * Tokens in the prompt
 */
export type Number_5 = number;
/**
 * Tokens in the completion
 */
export type Number_6 = number;
/**
 * Total tokens used
 */
export type Number_7 = number;
/**
 * Estimated cost for prompt tokens in dollars
 */
export type Number_8 = number;
/**
 * Estimated cost for completion tokens in dollars
 */
export type Number_9 = number;
/**
 * Total estimated cost in dollars
 */
export type Number_10 = number;

export interface CHAT_COMPLETIONOutput {
  id: String_29;
  model: String_30;
  content: String_31;
  finishReason: String_32;
  usage: Object_4;
  estimatedCost?: Object_5;
}
export interface Object_4 {
  promptTokens: Number_5;
  completionTokens: Number_6;
  totalTokens: Number_7;
}
/**
 * Estimated cost based on model pricing
 */
export interface Object_5 {
  prompt: Number_8;
  completion: Number_9;
  total: Number_10;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * Array of 2-5 model IDs to compare (e.g., ['openai/gpt-4o', 'anthropic/claude-3.5-sonnet', 'google/gemini-2.0-flash-exp'])
 *
 * @minItems 2
 * @maxItems 5
 */
export type Array_8 =
  | [String_33, String_33]
  | [String_33, String_33, String_33]
  | [String_33, String_33, String_33, String_33]
  | [String_33, String_33, String_33, String_33, String_33];
export type String_33 = string;
export type String_34 = "price" | "context_length" | "modality" | "moderation";
/**
 * Specific criteria to focus on in comparison. If not specified, all criteria are included.
 */
export type Array_9 = String_34[];

export interface COMPARE_MODELSInput {
  modelIds: Array_8;
  criteria?: Array_9;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

export type String_35 = string;
export type String_36 = string;
export type Array_10 = Object_6[];
/**
 * Automated recommendation based on comparison
 */
export type String_37 = string;

export interface COMPARE_MODELSOutput {
  comparison: Array_10;
  recommendation?: String_37;
}
export interface Object_6 {
  modelId: String_35;
  name: String_36;
  metrics: Object_7;
}
/**
 * Model metrics based on selected criteria
 */
export interface Object_7 {
  [k: string]: unknown;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * The model ID in format 'provider/model-name' (e.g., 'openai/gpt-4o', 'anthropic/claude-3.5-sonnet', 'google/gemini-2.0-flash-exp')
 */
export type String_38 = string;

export interface GET_MODELInput {
  modelId: String_38;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * Unique model identifier
 */
export type String_39 = string;
/**
 * Human-readable model name
 */
export type String_40 = string;
/**
 * Detailed model description
 */
export type String_41 = string;
/**
 * Maximum context length in tokens (includes prompt + completion)
 */
export type Number_11 = number;
/**
 * Cost per 1M prompt tokens in dollars
 */
export type String_42 = string;
/**
 * Cost per 1M completion tokens in dollars
 */
export type String_43 = string;
/**
 * Fixed cost per request (if applicable)
 */
export type String_44 = string;
/**
 * Cost per image input (if applicable)
 */
export type String_45 = string;
export type Number_12 = number;
/**
 * Maximum tokens in completion
 */
export type Number_13 = number;
/**
 * Whether provider moderates content
 */
export type Boolean_3 = boolean;
/**
 * Model capability (e.g., 'text->text', 'text+image->text')
 */
export type String_46 = string;
/**
 * Tokenizer used by the model
 */
export type String_47 = string;
/**
 * Instruction format if applicable
 */
export type String_48 = string;
export type String_49 = string;
export type String_50 = string;

export interface GET_MODELOutput {
  id: String_39;
  name: String_40;
  description?: String_41;
  contextLength: Number_11;
  pricing: Object_8;
  topProvider?: Object_9;
  architecture?: Object_10;
  perRequestLimits?: Object_11;
}
export interface Object_8 {
  prompt: String_42;
  completion: String_43;
  request?: String_44;
  image?: String_45;
}
/**
 * Information about the recommended provider
 */
export interface Object_9 {
  contextLength?: Number_12;
  maxCompletionTokens?: Number_13;
  isModerated: Boolean_3;
}
export interface Object_10 {
  modality: String_46;
  tokenizer: String_47;
  instructType?: String_48;
}
/**
 * Per-request token limits if applicable
 */
export interface Object_11 {
  promptTokens?: String_49;
  completionTokens?: String_50;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * Filter by model modality: 'text->text' for text-only, 'text+image->text' for vision models, 'text->image' for image generation
 */
export type String_51 = "text->text" | "text+image->text" | "text->image";
/**
 * Maximum price per 1M prompt tokens in dollars (e.g., 5 for $5)
 */
export type Number_14 = number;
/**
 * Minimum context length required in tokens (e.g., 100000 for 100k tokens)
 */
export type Number_15 = number;
/**
 * Search term to filter models by name, ID, or description (case-insensitive)
 */
export type String_52 = string;
/**
 * Sort results by: 'price' (cheapest first), 'context_length' (largest first), or 'name' (alphabetical)
 */
export type String_53 = "price" | "context_length" | "name";
/**
 * Maximum number of models to return (default: 50)
 */
export type Number_16 = number;

export interface LIST_MODELSInput {
  filter?: Object_12;
  sortBy?: String_53;
  limit?: Number_16;
}
/**
 * Optional filters to narrow down the model list
 */
export interface Object_12 {
  modality?: String_51;
  maxPromptPrice?: Number_14;
  minContextLength?: Number_15;
  search?: String_52;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * Unique model identifier (use this for API calls)
 */
export type String_54 = string;
/**
 * Human-readable model name
 */
export type String_55 = string;
/**
 * Model description
 */
export type String_56 = string;
/**
 * Maximum context length in tokens
 */
export type Number_17 = number;
/**
 * Cost per 1M prompt tokens in dollars (e.g., '0.50')
 */
export type String_57 = string;
/**
 * Cost per 1M completion tokens in dollars
 */
export type String_58 = string;
/**
 * Model capability (e.g., 'text->text', 'text+image->text')
 */
export type String_59 = string;
/**
 * Recommended provider for this model
 */
export type String_60 = string;
/**
 * Whether content is moderated
 */
export type Boolean_4 = boolean;
export type Array_11 = Object_13[];
/**
 * Total number of models matching filters
 */
export type Number_18 = number;
/**
 * Whether there are more results beyond the limit
 */
export type Boolean_5 = boolean;

export interface LIST_MODELSOutput {
  models: Array_11;
  total: Number_18;
  hasMore: Boolean_5;
}
export interface Object_13 {
  id: String_54;
  name: String_55;
  description?: String_56;
  contextLength: Number_17;
  promptPrice: String_57;
  completionPrice: String_58;
  modality: String_59;
  topProvider?: String_60;
  isModerated?: Boolean_4;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * Description of your task (e.g., 'generate Python code', 'write creative stories', 'analyze large documents', 'answer questions with images')
 */
export type String_61 = string;
/**
 * Maximum budget per 1M tokens in dollars (e.g., 5 for $5)
 */
export type Number_19 = number;
/**
 * Minimum required context length in tokens (e.g., 100000 for 100k)
 */
export type Number_20 = number;
/**
 * Required model capability: 'text->text' for text-only, 'text+image->text' for vision, 'text->image' for image generation
 */
export type String_62 = "text->text" | "text+image->text" | "text->image";
/**
 * What to prioritize: 'cost' for cheapest models, 'quality' for best performance, 'speed' for fastest models
 */
export type String_63 = "cost" | "quality" | "speed";

export interface RECOMMEND_MODELInput {
  taskDescription: String_61;
  requirements?: Object_14;
}
/**
 * Optional requirements and constraints for model selection
 */
export interface Object_14 {
  maxCostPer1MTokens?: Number_19;
  minContextLength?: Number_20;
  requiredModality?: String_62;
  prioritize?: String_63;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * Model identifier to use for API calls
 */
export type String_64 = string;
/**
 * Human-readable model name
 */
export type String_65 = string;
/**
 * Explanation of why this model is recommended for your task
 */
export type String_66 = string;
/**
 * Recommendation score (higher is better, 0-100 scale)
 */
export type Number_21 = number;
/**
 * Cost per 1M prompt tokens
 */
export type String_67 = string;
/**
 * Cost per 1M completion tokens
 */
export type String_68 = string;
/**
 * Maximum context length in tokens
 */
export type Number_22 = number;
/**
 * Model capability
 */
export type String_69 = string;
/**
 * Top recommended models ordered by score
 */
export type Array_12 = Object_15[];

export interface RECOMMEND_MODELOutput {
  recommendations: Array_12;
}
export interface Object_15 {
  modelId: String_64;
  name: String_65;
  reasoning: String_66;
  score: Number_21;
  pricing: Object_16;
  contextLength: Number_22;
  modality: String_69;
}
export interface Object_16 {
  promptPrice: String_67;
  completionPrice: String_68;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * Array of messages in the conversation
 *
 * @minItems 1
 */
export type Array_13 = [Object_17, ...Object_17[]];
export type String_70 = "user" | "assistant" | "system";
export type String_71 = string;
export type String_72 = string;
/**
 * Model ID or 'openrouter/auto' for automatic selection. Default: openrouter/auto
 */
export type String_73 = string;
export type String_74 = string;
/**
 * Fallback chain: array of model IDs to try in sequence
 */
export type Array_14 = String_74[];
/**
 * Sampling temperature (0-2)
 */
export type Number_23 = number;
/**
 * Maximum tokens to generate
 */
export type Number_24 = number;
/**
 * Nucleus sampling (0-1)
 */
export type Number_25 = number;
/**
 * Frequency penalty (-2 to 2)
 */
export type Number_26 = number;
/**
 * Presence penalty (-2 to 2)
 */
export type Number_27 = number;
export type String_75 = string;
export type String_76 = string;
export type Array_15 = String_76[];
export type String_77 = "price" | "throughput" | "latency";
export type String_78 = string;
export type Array_16 = String_78[];
export type String_79 = string;
export type Array_17 = String_79[];
export type Boolean_6 = boolean;
export type Boolean_7 = boolean;
/**
 * Unique user identifier
 */
export type String_80 = string;

export interface GET_STREAM_ENDPOINTInput {
  messages: Array_13;
  model?: String_73;
  models?: Array_14;
  temperature?: Number_23;
  maxTokens?: Number_24;
  topP?: Number_25;
  frequencyPenalty?: Number_26;
  presencePenalty?: Number_27;
  /**
   * Stop sequences
   */
  stop?: String_75 | Array_15;
  provider?: Object_18;
  user?: String_80;
}
export interface Object_17 {
  role: String_70;
  content: String_71;
  name?: String_72;
}
/**
 * Provider routing preferences
 */
export interface Object_18 {
  sort?: String_77;
  only?: Array_16;
  exclude?: Array_17;
  requireParameters?: Boolean_6;
  allowFallbacks?: Boolean_7;
}

/* eslint-disable */
/**
 * This file was automatically generated by json-schema-to-typescript.
 * DO NOT MODIFY IT BY HAND. Instead, modify the source JSONSchema file,
 * and run json-schema-to-typescript to regenerate this file.
 */

/**
 * SSE endpoint URL to connect for streaming (use EventSource or SSE client)
 */
export type String_81 = string;
/**
 * Unique session ID for this stream
 */
export type String_82 = string;
/**
 * ISO timestamp when this session expires (5 minutes from creation)
 */
export type String_83 = string;
/**
 * Instructions on how to consume the stream using SSE
 */
export type String_84 = string;

export interface GET_STREAM_ENDPOINTOutput {
  streamUrl: String_81;
  sessionId: String_82;
  expiresAt: String_83;
  instructions: String_84;
}

import { z } from "zod";

export type Mcp<T extends Record<string, (input: any) => Promise<any>>> = {
  [K in keyof T]: ((
    input: Parameters<T[K]>[0],
  ) => Promise<Awaited<ReturnType<T[K]>>>) & {
    asTool: () => Promise<{
      inputSchema: z.ZodType<Parameters<T[K]>[0]>;
      outputSchema?: z.ZodType<Awaited<ReturnType<T[K]>>>;
      description: string;
      id: string;
      execute: (
        input: Parameters<T[K]>[0],
      ) => Promise<Awaited<ReturnType<T[K]>>>;
    }>;
  };
};

export const StateSchema = z.object({});

export interface Env {
  DECO_CHAT_WORKSPACE: string;
  DECO_CHAT_API_JWT_PUBLIC_KEY: string;
  SELF: Mcp<{
    /**
     * OAuth for Deco Chat
     */
    DECO_CHAT_OAUTH_START: (
      input: DECO_CHAT_OAUTH_STARTInput,
    ) => Promise<DECO_CHAT_OAUTH_STARTOutput>;
    /**
     * Validate the state of the OAuth flow
     */
    DECO_CHAT_STATE_VALIDATION: (
      input: DECO_CHAT_STATE_VALIDATIONInput,
    ) => Promise<DECO_CHAT_STATE_VALIDATIONOutput>;
    /**
     * List views exposed by this MCP
     */
    DECO_CHAT_VIEWS_LIST: (
      input: DECO_CHAT_VIEWS_LISTInput,
    ) => Promise<DECO_CHAT_VIEWS_LISTOutput>;
    /**
     * Get the current logged in user
     */
    GET_USER: (input: GET_USERInput) => Promise<GET_USEROutput>;
    /**
     * Send a non-streaming chat completion request to OpenRouter. Supports single model selection, automatic model routing (openrouter/auto), fallback chains, and provider preferences. Returns the complete response when generation is finished, including token usage and cost estimation. Perfect for standard chat interactions where you don't need real-time streaming.
     */
    CHAT_COMPLETION: (
      input: CHAT_COMPLETIONInput,
    ) => Promise<CHAT_COMPLETIONOutput>;
    /**
     * Compare multiple OpenRouter models side-by-side to help choose the best model for a specific use case. Compares pricing (prompt and completion costs), context length, capabilities (modality), and performance characteristics. Returns a detailed comparison table and an automatic recommendation. Useful when deciding between multiple models for a task.
     */
    COMPARE_MODELS: (
      input: COMPARE_MODELSInput,
    ) => Promise<COMPARE_MODELSOutput>;
    /**
     * Get detailed information about a specific OpenRouter model including pricing, capabilities, context length, provider information, and supported features. Use this to learn about a model before using it for chat completions. Model IDs follow the format 'provider/model-name' (e.g., 'openai/gpt-4o', 'anthropic/claude-3.5-sonnet').
     */
    GET_MODEL: (input: GET_MODELInput) => Promise<GET_MODELOutput>;
    /**
     * List all available models from OpenRouter with their details, pricing, and capabilities. Returns comprehensive information about each model including context length, pricing per 1M tokens, modality (text, vision, etc.), and provider information. Supports filtering by price, context length, modality, and search terms. Perfect for discovering and comparing available AI models.
     */
    LIST_MODELS: (input: LIST_MODELSInput) => Promise<LIST_MODELSOutput>;
    /**
     * Get intelligent model recommendations based on your task description and requirements. The system analyzes your task (e.g., 'code generation', 'creative writing', 'data analysis') and suggests the most suitable models considering cost, quality, context length, and capabilities. Each recommendation includes detailed reasoning explaining why the model is suitable. Perfect for discovering the right model when you're not sure which to use.
     */
    RECOMMEND_MODEL: (
      input: RECOMMEND_MODELInput,
    ) => Promise<RECOMMEND_MODELOutput>;
    /**
     * Prepare a streaming chat completion session. Returns a URL that can be used to stream responses via Server-Sent Events (SSE). The stream URL is authenticated and pre-configured with all parameters. Perfect for real-time chat applications where you want to display responses as they're generated. Note: MCP doesn't support streaming in tool responses, so you'll need to connect to the URL directly to consume the stream.
     */
    GET_STREAM_ENDPOINT: (
      input: GET_STREAM_ENDPOINTInput,
    ) => Promise<GET_STREAM_ENDPOINTOutput>;
  }>;
}

export const Scopes = {};
